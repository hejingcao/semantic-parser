base_path = 'output/model.RD.1'

train_paths = ['data/v1/deepbank1.1-eds.standard.train']
dev_path = 'data/v1/deepbank1.1-eds.standard.best.CI.dev'
dev_smatch_path = 'data/v1/deepbank1.1-eds.smatch.dev'
lemma_dictionary_path = 'data/v1/deepbank1.1-eds.mrp.4.lemma.pkl'

vocab_path = 'output/v1/vocabs.CI.word_threshold=1.txt'

max_steps = 200000
log_frequency = 50
checkpoint_frequency = 500
num_old_checkpoints = 0

hyper_params.bucket.batch_item_count = 5000

hyper_params.pretrained_encoder.type = 'bert'
hyper_params.pretrained_encoder.bert_options.bert_path = '../../../data/bert/bert-base-cased'
hyper_params.pretrained_encoder.bert_options.subword_separator = '_'
hyper_params.pretrained_encoder.bert_options.project_to = 500

hyper_params.sentence_embedding.word_size = 500
hyper_params.sentence_embedding.word_dropout = 0.2
hyper_params.sentence_embedding.char_size = 0
hyper_params.sentence_embedding.mode = 'add'
hyper_params.sentence_embedding.use_layer_norm = False

hyper_params.encoder.type = 'lstm'
hyper_params.encoder.lstm_options.num_layers = 4
hyper_params.encoder.lstm_options.hidden_size = 500
hyper_params.encoder.lstm_options.input_keep_prob = 0.80

hyper_params.optimizer.type = 'adamw'
hyper_params.label_loss_type = 'label_smoothing'

hyper_params.advanced_learning.lr_reduce_method = 'cosine_with_hard_restarts'
hyper_params.advanced_learning.restart_cycles = 5